name: Deploy to Kubernetes

concurrency:
  group: ${{ github.workflow }}-${{ github.event.inputs.environment || github.ref }}
  cancel-in-progress: true

on:
  push:
    branches:
      - main      # Deploy to staging
      - release/* # Deploy to production
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        type: choice
        options:
          - staging
          - production
      image_tag:
        description: 'Image tag to deploy (default: latest commit SHA)'
        required: false
        type: string

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  determine-environment:
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.env.outputs.environment }}
      image_tag: ${{ steps.env.outputs.image_tag }}
    steps:
      - name: Determine deployment environment
        id: env
        run: |
          if [[ "${{ github.event.inputs.environment }}" != "" ]]; then
            echo "environment=${{ github.event.inputs.environment }}" >> $GITHUB_OUTPUT
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "environment=staging" >> $GITHUB_OUTPUT
          elif [[ "${{ github.ref }}" == refs/heads/release/* ]]; then
            echo "environment=production" >> $GITHUB_OUTPUT
          else
            echo "environment=staging" >> $GITHUB_OUTPUT
          fi
          
          if [[ "${{ github.event.inputs.image_tag }}" != "" ]]; then
            echo "image_tag=${{ github.event.inputs.image_tag }}" >> $GITHUB_OUTPUT
          elif [[ "${{ github.ref_type }}" == "tag" ]]; then # Check if the ref is a tag
            echo "image_tag=${{ github.ref_name }}" >> $GITHUB_OUTPUT # Use the tag name
          else
            echo "image_tag=${{ github.sha }}" >> $GITHUB_OUTPUT # Default to commit SHA
          fi

  deploy-staging:
    needs: determine-environment
    if: needs.determine-environment.outputs.environment == 'staging'
    runs-on: ubuntu-latest
    environment: 
      name: staging
      url: https://staging.mvp-auth.example.com
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4.2.2

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4.0.0
        with:
          version: 'v1.28.0'

      - name: Set up Kustomize
        run: |
          curl -Lo kustomize.tar.gz "https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv5.6.0/kustomize_v5.6.0_linux_amd64.tar.gz"
          tar -xzf kustomize.tar.gz
          sudo mv kustomize /usr/local/bin/

      - name: Configure Kubernetes context
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBE_CONFIG_STAGING }}" | base64 --decode > $HOME/.kube/config
          chmod 600 $HOME/.kube/config

      - name: Verify cluster connection
        run: |
          kubectl cluster-info
          kubectl get nodes
          echo "âœ… Connected to staging cluster"

      - name: Update image tags in manifests
        run: |
          cd kubernetes/apps/zamaz/overlays/staging
          kustomize edit set image auth-service=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/auth-service:${{ needs.determine-environment.outputs.image_tag }}
          kustomize edit set image frontend=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:${{ needs.determine-environment.outputs.image_tag }}

      - name: Validate manifests
        run: |
          kustomize build kubernetes/apps/zamaz/overlays/staging | kubectl apply --dry-run=client -f -
          echo "âœ… Manifests validation passed"

      - name: Deploy to staging
        run: |
          echo "ðŸš€ Deploying to staging environment..."
          ./scripts/deploy-k8s.sh staging
        env:
          IMAGE_TAG: ${{ needs.determine-environment.outputs.image_tag }}

      - name: Wait for deployment
        run: |
          echo "â³ Waiting for deployments to be ready..."
          kubectl wait --for=condition=available --timeout=300s deployment/auth-service -n staging
          kubectl wait --for=condition=available --timeout=300s deployment/frontend -n staging
          echo "âœ… All deployments are ready"

      - name: Run smoke tests
        run: |
          echo "ðŸ§ª Running smoke tests..."
          
          # Test auth service health
          kubectl run staging-smoke-test-auth --image=curlimages/curl:latest --rm -it --restart=Never -n staging -- \
            curl -f http://auth-service.staging.svc.cluster.local:8080/health || exit 1
          
          # Test frontend accessibility
          kubectl run staging-smoke-test-frontend --image=curlimages/curl:latest --rm -it --restart=Never -n staging -- \
            curl -f http://frontend.staging.svc.cluster.local:3000/ || exit 1

          # Test auth service metrics endpoint
          kubectl run staging-smoke-test-auth-metrics --image=curlimages/curl:latest --rm -it --restart=Never -n staging -- \
            curl -f http://auth-service.staging.svc.cluster.local:9000/metrics || exit 1
          
          echo "âœ… Smoke tests passed"

      - name: Verify all pods are running in staging
        run: |
          echo "Verifying pod statuses in staging namespace..."
          failed_pods=$(kubectl get pods -n staging --field-selector=status.phase!=Running --no-headers | wc -l)
          if [ $failed_pods -ne 0 ]; then
            echo "âŒ Some pods in staging are not running"
            kubectl get pods -n staging
            exit 1
          fi
          echo "âœ… All pods in staging are running."

      - name: Get deployment status
        run: |
          echo "ðŸ“Š Deployment Status:"
          kubectl get pods -n staging
          kubectl get services -n staging
          kubectl get ingress -n staging

  deploy-production:
    needs: determine-environment
    if: needs.determine-environment.outputs.environment == 'production'
    runs-on: ubuntu-latest
    environment: 
      name: production
      url: https://mvp-auth.example.com
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4.2.2

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4.0.0
        with:
          version: 'v1.28.0'

      - name: Set up Kustomize
        run: |
          curl -Lo kustomize.tar.gz "https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv5.6.0/kustomize_v5.6.0_linux_amd64.tar.gz"
          tar -xzf kustomize.tar.gz
          sudo mv kustomize /usr/local/bin/

      - name: Configure Kubernetes context
        run: |
          mkdir -p $HOME/.kube
          echo "${{ secrets.KUBE_CONFIG_PRODUCTION }}" | base64 --decode > $HOME/.kube/config
          chmod 600 $HOME/.kube/config

      - name: Verify cluster connection
        run: |
          kubectl cluster-info
          kubectl get nodes
          echo "âœ… Connected to production cluster"

      - name: Pre-deployment backup
        run: |
          echo "ðŸ’¾ Creating backup of current deployment..."
          kubectl get deployment auth-service -n production -o yaml > backup-auth-deployment.yaml
          kubectl get deployment frontend -n production -o yaml > backup-frontend-deployment.yaml
          echo "âœ… Backup created"

      - name: Update image tags in manifests
        run: |
          cd kubernetes/apps/zamaz/overlays/production
          kustomize edit set image auth-service=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/auth-service:${{ needs.determine-environment.outputs.image_tag }}
          kustomize edit set image frontend=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/frontend:${{ needs.determine-environment.outputs.image_tag }}

      - name: Validate manifests
        run: |
          kustomize build kubernetes/apps/zamaz/overlays/production | kubectl apply --dry-run=client -f -
          echo "âœ… Manifests validation passed"

      - name: Deploy to production
        run: |
          echo "ðŸš€ Deploying to production environment..."
          ./scripts/deploy-k8s.sh production
        env:
          IMAGE_TAG: ${{ needs.determine-environment.outputs.image_tag }}

      - name: Wait for deployment with extended timeout
        run: |
          echo "â³ Waiting for deployments to be ready..."
          kubectl wait --for=condition=available --timeout=600s deployment/auth-service -n production
          kubectl wait --for=condition=available --timeout=600s deployment/frontend -n production
          echo "âœ… All deployments are ready"

      - name: Run comprehensive health checks
        run: |
          echo "ðŸ§ª Running comprehensive health checks..."
          
          # Test auth service health
          kubectl run production-health-check-auth --image=curlimages/curl:latest --rm -it --restart=Never -n production -- \
            curl -f http://auth-service.production.svc.cluster.local:8080/health || exit 1
          
          # Test frontend accessibility
          kubectl run production-health-check-frontend --image=curlimages/curl:latest --rm -it --restart=Never -n production -- \
            curl -f http://frontend.production.svc.cluster.local:3000/ || exit 1
          
          # Test metrics endpoints
          kubectl run production-health-check-auth-metrics --image=curlimages/curl:latest --rm -it --restart=Never -n production -- \
            curl -f http://auth-service.production.svc.cluster.local:9000/metrics || exit 1
          
          echo "âœ… All health checks passed"

      - name: Verify HPA and scaling
        run: |
          echo "ðŸ“Š Checking Horizontal Pod Autoscaler..."
          kubectl get hpa -n production
          kubectl top nodes
          kubectl top pods -n production

      - name: Post-deployment verification
        run: |
          echo "ðŸ” Final deployment verification:"
          kubectl get pods -n production
          kubectl get services -n production
          kubectl get ingress -n production
          kubectl get hpa -n production
          
          # Check that all pods are running
          failed_pods=$(kubectl get pods -n production --field-selector=status.phase!=Running --no-headers | wc -l)
          if [ $failed_pods -ne 0 ]; then
            echo "âŒ Some pods are not running"
            kubectl get pods -n production
            exit 1
          fi
          
          echo "âœ… Production deployment successful!"

      - name: Notify deployment success
        if: success()
        run: |
          echo "ðŸŽ‰ Production deployment completed successfully!"
          echo "ðŸ“Š Deployment Summary:"
          echo "- Environment: production"
          echo "- Image Tag: ${{ needs.determine-environment.outputs.image_tag }}"
          echo "- Commit: ${{ github.sha }}"
          echo "- Actor: ${{ github.actor }}"

  rollback:
    if: (needs.deploy-staging.result == 'failure' || needs.deploy-production.result == 'failure')
    needs: [determine-environment, deploy-staging, deploy-production]
    runs-on: ubuntu-latest
    steps:
      - name: Configure kubectl for Rollback
        run: |
          mkdir -p $HOME/.kube
          chmod 700 $HOME/.kube
          NAMESPACE=""
          if [[ "${{ needs.determine-environment.outputs.environment }}" == "staging" ]]; then
            NAMESPACE="staging"
            echo "${{ secrets.KUBE_CONFIG_STAGING }}" | base64 --decode > $HOME/.kube/config
          elif [[ "${{ needs.determine-environment.outputs.environment }}" == "production" ]]; then
            NAMESPACE="production"
            echo "${{ secrets.KUBE_CONFIG_PRODUCTION }}" | base64 --decode > $HOME/.kube/config
          else
            echo "::error::Unknown environment for rollback: ${{ needs.determine-environment.outputs.environment }}"
            exit 1
          fi
          chmod 600 $HOME/.kube/config
          echo "Kubernetes context configured for $NAMESPACE"

      - name: Set up kubectl CLI
        uses: azure/setup-kubectl@v4.0.0
        with:
          version: 'v1.28.0' # Use a specific version consistent with deploy jobs

      - name: Attempt Rollback
        run: |
          NAMESPACE=""
          if [[ "${{ needs.determine-environment.outputs.environment }}" == "staging" ]]; then
            NAMESPACE="staging"
          elif [[ "${{ needs.determine-environment.outputs.environment }}" == "production" ]]; then
            NAMESPACE="production"
          else
            # This case should ideally be caught by the previous step, but as a safeguard:
            echo "::error::Unknown environment, cannot determine namespace for rollback."
            exit 1
          fi

          echo "Attempting rollback for environment: $NAMESPACE"
          echo "Failed Image Tag: ${{ needs.determine-environment.outputs.image_tag }}"

          # Rollback deployments
          if kubectl rollout undo deployment/auth-service -n $NAMESPACE; then
            echo "âœ… Rollback initiated for auth-service in $NAMESPACE"
          else
            echo "::error::Failed to initiate rollback for auth-service in $NAMESPACE. Check deployment name and namespace."
          fi

          if kubectl rollout undo deployment/frontend -n $NAMESPACE; then
            echo "âœ… Rollback initiated for frontend in $NAMESPACE"
          else
            echo "::error::Failed to initiate rollback for frontend in $NAMESPACE. Check deployment name and namespace."
          fi

          echo "Waiting a few seconds for rollback to apply..."
          sleep 15 # Give some time for the rollback to take effect

          echo "Verifying pods after rollback attempt in $NAMESPACE namespace..."
          kubectl get pods -n $NAMESPACE

          echo "Verifying rollout status of deployments in $NAMESPACE namespace..."
          # Check status of specific deployments
          kubectl rollout status deployment/auth-service -n $NAMESPACE --timeout=60s || echo "::warning::Rollback status check for auth-service failed or timed out."
          kubectl rollout status deployment/frontend -n $NAMESPACE --timeout=60s || echo "::warning::Rollback status check for frontend failed or timed out."

          echo "Rollback process completed. Manual verification recommended."
